# ğŸ¤ WhisperSilent - Advanced Real-time Transcription System

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Platform](https://img.shields.io/badge/platform-linux%20%7C%20macos%20%7C%20raspberry--pi-lightgrey.svg)](https://github.com/whispersilent)

**ğŸš€ Professional-grade real-time transcription system with advanced features**

Comprehensive audio transcription platform featuring **speaker identification**, **real-time WebSocket streaming**, **hourly aggregation**, and a complete **REST API**. Optimized for Raspberry Pi and embedded devices.

## âœ¨ Key Features

### ğŸ¯ Core Transcription
- ğŸ¤ **Real-time audio capture** using ALSA/PortAudio
- ğŸ§  **12+ speech recognition engines** (Google, Whisper, Vosk, Azure, etc.)
- ğŸ”„ **Online & Offline** processing capabilities  
- ğŸ“Š **Quality control** with confidence scoring

### ğŸ­ Speaker Identification (Feature Toggle)
- ğŸ‘¥ **Multi-speaker detection** and tracking
- ğŸ§  **4 identification methods**: Simple Energy, PyAnnote, Resemblyzer, SpeechBrain
- ğŸ“ˆ **Speaker profiles** with confidence scoring
- ğŸ”§ **Configurable thresholds** and parameters

### ğŸ“¡ Real-time WebSocket API
- âš¡ **Live transcription streaming** to multiple clients
- ğŸ”” **Event subscriptions**: transcriptions, speaker changes, system status
- ğŸŒ **Multi-client support** (up to 50 concurrent connections)
- ğŸ“± **Web and Python clients** included

### â° Intelligent Aggregation  
- ğŸ“‹ **Hourly text aggregation** to reduce API load
- ğŸ”‡ **Silence gap detection** (5+ minutes triggers finalization)
- ğŸ“Š **Rich metadata** tracking (word counts, processing times)
- ğŸ” **Partial queries** during aggregation

### ğŸŒ Complete REST API
- ğŸ¥ **Health monitoring** with CPU/memory metrics
- ğŸ“ **Transcription management** and search
- ğŸ›ï¸ **System control** (start/stop, configuration)
- ğŸ“– **Interactive Swagger documentation**

### ğŸ”§ Professional Features
- ğŸ“ˆ **Performance monitoring** and analytics
- ğŸ’¾ **Persistent storage** with chronological organization
- ğŸ”„ **Automatic retry logic** for API calls
- ğŸš€ **Raspberry Pi optimized** with architecture detection

## ğŸ“‹ System Requirements

### Hardware
- **CPU**: Raspberry Pi 2W+ or x86_64 (ARM64, ARMv7 supported)
- **Memory**: 1GB RAM minimum, 2GB+ recommended
- **Storage**: 5GB available space
- **Audio**: USB microphone or Seeed VoiceCard

### Software
- **OS**: Linux (Ubuntu/Debian), macOS, Windows
- **Python**: 3.8+ with pip and venv
- **Internet**: Optional (for online engines and model downloads)

## âš¡ Quick Installation

### Automated Setup (Recommended)

```bash
# Clone repository
git clone https://github.com/your-username/whispersilent.git
cd whispersilent

# Run complete installation with testing
chmod +x scripts/install_and_test.sh
./scripts/install_and_test.sh
```

The installation script will:
- âœ… Install system dependencies
- âœ… Set up Python virtual environment
- âœ… Download and compile optimized binaries
- âœ… Download transcription models
- âœ… Run comprehensive validation tests
- âœ… Configure audio devices automatically

### Manual Installation

```bash
# 1. System dependencies (Ubuntu/Debian)
sudo apt update && sudo apt install -y \\
    python3 python3-pip python3-venv \\
    build-essential cmake git wget \\
    portaudio19-dev alsa-utils

# 2. Python environment
python3 -m venv venv
source venv/bin/activate

# 3. Python dependencies
pip install -r requirements.txt

# 4. Configure environment
cp .env.example .env
nano .env

# 5. Download models and test
python3 scripts/validate_installation.py
```

## ğŸ”§ Configuration

### Basic Configuration (.env)

```bash
# === CORE AUDIO SETTINGS ===
AUDIO_DEVICE=auto                    # Auto-detect best microphone
SAMPLE_RATE=16000
CHANNELS=1

# === SPEECH RECOGNITION ===
SPEECH_RECOGNITION_ENGINE=google     # Free Google API (default)
SPEECH_RECOGNITION_LANGUAGE=pt-BR
SPEECH_RECOGNITION_TIMEOUT=30

# === API INTEGRATION (OPTIONAL) ===
API_ENDPOINT=https://your-api.com/transcribe
API_KEY=your_api_key

# === HTTP SERVER ===
HTTP_HOST=localhost
HTTP_PORT=8080

# === FEATURE TOGGLES ===
SPEAKER_IDENTIFICATION_ENABLED=false
REALTIME_API_ENABLED=false
LOG_LEVEL=INFO
```

### Advanced Features

#### Speaker Identification

```bash
# Enable speaker identification
SPEAKER_IDENTIFICATION_ENABLED=true
SPEAKER_IDENTIFICATION_METHOD=simple_energy

# Available methods:
# - simple_energy: Basic energy-based (no dependencies)
# - pyannote: Neural speaker diarization (requires model)
# - resemblyzer: Speaker embeddings (pip install resemblyzer)
# - speechbrain: Advanced recognition (pip install speechbrain)

# Fine-tuning
SPEAKER_CONFIDENCE_THRESHOLD=0.7
SPEAKER_MIN_SEGMENT_DURATION=2.0
SPEAKER_MAX_SPEAKERS=10
```

#### Real-time WebSocket API

```bash
# Enable real-time streaming
REALTIME_API_ENABLED=true
REALTIME_WEBSOCKET_PORT=8081
REALTIME_MAX_CONNECTIONS=50
REALTIME_BUFFER_SIZE=100
```

#### Speech Recognition Engines

**Online Engines** (require internet):
```bash
# Google (free, default)
SPEECH_RECOGNITION_ENGINE=google

# OpenAI Whisper API (excellent quality)
SPEECH_RECOGNITION_ENGINE=whisper_api
OPENAI_API_KEY=your_openai_key

# Google Cloud (high quality)
SPEECH_RECOGNITION_ENGINE=google_cloud
GOOGLE_CLOUD_CREDENTIALS_JSON=path/to/credentials.json

# Azure, IBM, Wit.ai, Houndify, Groq
SPEECH_RECOGNITION_ENGINE=azure
AZURE_SPEECH_KEY=your_azure_key
```

**Offline Engines** (work without internet):
```bash
# Vosk (good quality, requires model download)
SPEECH_RECOGNITION_ENGINE=vosk
VOSK_MODEL_PATH=path/to/vosk-model

# Local Whisper (best quality, high CPU)
SPEECH_RECOGNITION_ENGINE=whisper_local
WHISPER_MODEL=base

# CMU Sphinx (lightweight)
SPEECH_RECOGNITION_ENGINE=sphinx
```

## ğŸš€ Usage

### Start the System

```bash
# Complete system with HTTP server and real-time API
python3 src/mainWithServer.py

# Basic transcription only
python3 src/main.py
```

### Access Interfaces

- **ğŸ¥ Health Dashboard**: http://localhost:8080/health
- **ğŸ“– API Documentation**: http://localhost:8080/api-docs  
- **ğŸ“ Transcriptions**: http://localhost:8080/transcriptions
- **ğŸ“¡ Real-time Client**: Open `examples/realtime_web_client.html`
- **ğŸ”Œ WebSocket**: `ws://localhost:8081`

### Testing & Validation

```bash
# Quick microphone test (2 seconds)
python3 scripts/test_microphone_basic.py

# Test all available speech engines
python3 scripts/test_transcription_api.py

# Complete system integration test (20+ seconds)
python3 scripts/test_complete_system.py

# Full installation validation
python3 scripts/validate_installation.py

# Audio device detection and configuration
python3 scripts/detect_audio_devices.py --auto --update
```

## ğŸŒ API Reference

### REST API Endpoints

#### Health & Monitoring
```bash
GET /health                    # Basic health status
GET /health/detailed          # Comprehensive system metrics
GET /status                   # Pipeline status and configuration
```

#### Transcription Management
```bash
GET /transcriptions                              # List all transcriptions
GET /transcriptions?limit=50                     # Last 50 transcriptions
GET /transcriptions?recent_minutes=60            # Last hour
GET /transcriptions/search?q=keyword             # Search transcriptions
GET /transcriptions/statistics                   # Get statistics
GET /transcriptions/{id}                         # Specific transcription
POST /transcriptions/export                      # Export to JSON
POST /transcriptions/send-unsent                 # Send pending to API
```

#### Hourly Aggregation
```bash
GET /aggregation/status                          # Current aggregation status
GET /aggregation/texts                           # List aggregated texts
GET /aggregation/texts/{hour_timestamp}          # Specific hour
POST /aggregation/finalize                       # Force finalization
POST /aggregation/toggle?enabled=true           # Enable/disable
GET /aggregation/statistics                      # Aggregation stats
```

#### Speaker Identification
```bash
GET /speakers/profiles                           # All speaker profiles
GET /speakers/statistics                         # Speaker identification stats
PUT /speakers/{id}/name                          # Update speaker name
DELETE /speakers/{id}                            # Remove speaker profile
POST /speakers/toggle?enabled=true               # Enable/disable feature
```

#### System Control
```bash
POST /control/start                              # Start transcription pipeline
POST /control/stop                               # Stop transcription pipeline
POST /control/toggle-api-sending                 # Toggle external API calls
```

#### Real-time API
```bash
GET /realtime/status                             # WebSocket server status
GET /realtime/statistics                         # Real-time API statistics
```

### WebSocket API

#### Connection
```javascript
const ws = new WebSocket('ws://localhost:8081');

ws.onmessage = function(event) {
    const data = JSON.parse(event.data);
    console.log('Event:', data);
};
```

#### Event Types
- **transcription**: New transcription text
- **speaker_change**: Speaker identification update  
- **chunk_processed**: Audio processing status
- **heartbeat**: Server status and metrics
- **error**: System errors and warnings

#### Client Commands
```javascript
// Subscribe to events
ws.send(JSON.stringify({
    action: 'subscribe',
    events: ['transcription', 'speaker_change']
}));

// Send heartbeat
ws.send(JSON.stringify({
    action: 'ping',
    timestamp: Date.now() / 1000
}));

// Request recent events
ws.send(JSON.stringify({
    action: 'get_buffer'
}));
```

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Microphone  â”‚â”€â”€â”€â–¶â”‚ AudioCapture â”‚â”€â”€â”€â–¶â”‚ AudioProcessor  â”‚
â”‚ (USB/ALSA)  â”‚    â”‚   (Thread)   â”‚    â”‚ (VAD/Chunking)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Speaker ID   â”‚â—€â”€â”€â”€â”‚ Transcriptionâ”‚â—€â”€â”€â”€â”‚ SpeechRecognitionâ”‚
â”‚Service      â”‚    â”‚ Pipeline     â”‚    â”‚ Service (12+)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                     â”‚
                            â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Hourly       â”‚    â”‚Health        â”‚    â”‚ Storage &       â”‚
â”‚Aggregator   â”‚    â”‚Monitor       â”‚    â”‚ File Management â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Real-time    â”‚    â”‚HTTP Server   â”‚    â”‚ External API    â”‚
â”‚WebSocket APIâ”‚    â”‚(REST API)    â”‚    â”‚ (Optional)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

1. **AudioCapture**: ALSA/PortAudio interface with automatic device detection
2. **AudioProcessor**: Voice activity detection and intelligent segmentation
3. **SpeechRecognitionService**: Unified interface for 12+ recognition engines
4. **SpeakerIdentificationService**: Multi-method speaker identification (feature toggle)
5. **HourlyAggregator**: Intelligent text aggregation with silence detection
6. **RealtimeAPI**: WebSocket server for live transcription streaming
7. **TranscriptionPipeline**: Main orchestrator with health monitoring
8. **HTTPServer**: Complete REST API with Swagger documentation
9. **HealthMonitor**: System performance and resource monitoring

## ğŸ“Š Performance & Monitoring

### Health Metrics

```json
{
  "status": "healthy",
  "timestamp": 1704067200.0,
  "uptime_seconds": 3600,
  "summary": {
    "pipeline_running": true,
    "total_transcriptions": 150,
    "cpu_usage": 25.5,
    "memory_usage": 65.2,
    "recent_errors_count": 0,
    "api_success_rate": 98.5
  },
  "system_metrics": {
    "cpu_percent": 25.5,
    "memory_percent": 65.2,
    "process_memory_mb": 245.7,
    "process_threads": 8,
    "disk_usage_percent": 42.1
  }
}
```

### Performance Benchmarks (Raspberry Pi 4)

| Model | Size | Latency | CPU Usage | Memory |
|-------|------|---------|-----------|---------|
| tiny | 39 MB | 0.5-1s | 30% | 150MB |
| base | 142 MB | 1-2s | 50% | 250MB |
| small | 466 MB | 2-4s | 70% | 400MB |

### Real-time Analytics

- ğŸ“ˆ **Processing Times**: Average transcription latency
- ğŸ¯ **Success Rates**: Transcription and API call success
- ğŸ’¾ **Resource Usage**: CPU, memory, disk monitoring
- ğŸ‘¥ **Speaker Analytics**: Speaker identification accuracy
- ğŸŒ **Connection Health**: WebSocket client monitoring

## ğŸ“ Project Structure

```
whispersilent/
â”œâ”€â”€ ğŸ“ src/                          # Source code
â”‚   â”œâ”€â”€ ğŸ main.py                   # Basic entry point
â”‚   â”œâ”€â”€ ğŸŒ mainWithServer.py         # Full system entry point
â”‚   â”œâ”€â”€ ğŸ“ core/                     # Core components
â”‚   â”‚   â”œâ”€â”€ config.py               # Centralized configuration
â”‚   â”‚   â”œâ”€â”€ logger.py               # Logging system
â”‚   â”‚   â”œâ”€â”€ audioCapture.py         # Audio capture interface
â”‚   â”‚   â”œâ”€â”€ audioProcessor.py       # Audio processing
â”‚   â”‚   â””â”€â”€ audioDeviceDetector.py  # Device auto-detection
â”‚   â”œâ”€â”€ ğŸ“ transcription/            # Transcription services
â”‚   â”‚   â”œâ”€â”€ speechRecognitionService.py     # 12+ engines
â”‚   â”‚   â”œâ”€â”€ transcriptionPipeline.py        # Main orchestrator
â”‚   â”‚   â”œâ”€â”€ transcriptionStorage.py         # In-memory storage
â”‚   â”‚   â””â”€â”€ transcriptionFiles.py           # File management
â”‚   â”œâ”€â”€ ğŸ“ api/                      # API services
â”‚   â”‚   â”œâ”€â”€ httpServer.py           # REST API server
â”‚   â”‚   â”œâ”€â”€ realtimeAPI.py          # WebSocket server
â”‚   â”‚   â”œâ”€â”€ apiService.py           # External API client
â”‚   â”‚   â””â”€â”€ swagger.py              # API documentation
â”‚   â””â”€â”€ ğŸ“ services/                 # Advanced services
â”‚       â”œâ”€â”€ healthMonitor.py        # System monitoring
â”‚       â”œâ”€â”€ hourlyAggregator.py     # Text aggregation
â”‚       â””â”€â”€ speakerIdentification.py # Speaker features
â”œâ”€â”€ ğŸ“ scripts/                      # Automation & testing
â”‚   â”œâ”€â”€ ğŸ”§ install_and_test.sh      # Complete installation
â”‚   â”œâ”€â”€ ğŸ§ª test_complete_system.py  # Integration testing
â”‚   â”œâ”€â”€ ğŸ¤ test_microphone_basic.py # Quick audio test
â”‚   â”œâ”€â”€ ğŸ“¡ test_transcription_api.py # Engine testing
â”‚   â””â”€â”€ ğŸ” validate_installation.py # Installation validator
â”œâ”€â”€ ğŸ“ examples/                     # Client examples
â”‚   â”œâ”€â”€ ğŸ realtime_client.py       # Python WebSocket client
â”‚   â””â”€â”€ ğŸŒ realtime_web_client.html # Web interface
â”œâ”€â”€ ğŸ“ docs/                         # Documentation
â”‚   â”œâ”€â”€ ğŸ“ api/                      # API documentation
â”‚   â”œâ”€â”€ ğŸ“ installation/             # Setup guides
â”‚   â”œâ”€â”€ ğŸ“ features/                 # Feature documentation
â”‚   â””â”€â”€ ğŸ“ examples/                 # Usage examples
â”œâ”€â”€ ğŸ“ models/                       # AI models (downloaded)
â”œâ”€â”€ ğŸ“ transcriptions/               # Output data
â”œâ”€â”€ ğŸ“ tests/                        # Test suite
â”œâ”€â”€ ğŸ”§ requirements.txt              # Python dependencies
â”œâ”€â”€ âš™ï¸ .env.example                  # Configuration template
â”œâ”€â”€ ğŸ“‹ .gitignore                    # Git ignore rules
â””â”€â”€ ğŸ“– README.md                     # This file
```

## ğŸ”§ Advanced Configuration

### Raspberry Pi Optimization

```bash
# Lightweight model for Pi 2W
WHISPER_MODEL_PATH=./models/ggml-tiny.bin

# Reduced chunk size for lower latency
CHUNK_DURATION_MS=2000

# Optimized thread count
WHISPER_THREADS=2

# Energy-based speaker detection (no AI dependencies)
SPEAKER_IDENTIFICATION_METHOD=simple_energy
```

### High-Quality Setup

```bash
# Best quality model
WHISPER_MODEL_PATH=./models/ggml-large-v3.bin

# OpenAI Whisper API for cloud processing
SPEECH_RECOGNITION_ENGINE=whisper_api
OPENAI_API_KEY=your_openai_key

# Advanced speaker identification
SPEAKER_IDENTIFICATION_ENABLED=true
SPEAKER_IDENTIFICATION_METHOD=pyannote
HUGGINGFACE_TOKEN=your_hf_token
```

### Production Environment

```bash
# Bind to specific interface
HTTP_HOST=0.0.0.0
HTTP_PORT=8080

# Enable real-time features
REALTIME_API_ENABLED=true
REALTIME_WEBSOCKET_PORT=8081

# Performance logging
LOG_LEVEL=INFO
LOG_FILE=logs/production.log

# Resource limits
REALTIME_MAX_CONNECTIONS=100
```

## ğŸ”’ Security & Deployment

### Security Best Practices

- ğŸ” **Environment Variables**: All sensitive data in `.env`
- ğŸŒ **Local Binding**: Default localhost-only access
- ğŸ“ **Safe Logging**: No sensitive data in logs
- ğŸ§¹ **Auto Cleanup**: Temporary files automatically removed
- ğŸ”’ **User Permissions**: Run as non-root user

### SystemD Service

```bash
# Install as system service (included in installation)
sudo systemctl enable whispersilent
sudo systemctl start whispersilent
sudo systemctl status whispersilent

# View logs
sudo journalctl -u whispersilent -f
```

### Docker Deployment

```bash
# Build image
docker build -t whispersilent .

# Run with audio device access
docker run -d --name whispersilent \\
  --device /dev/snd \\
  -p 8080:8080 -p 8081:8081 \\
  -v $(pwd)/transcriptions:/app/transcriptions \\
  -v $(pwd)/.env:/app/.env \\
  whispersilent
```

## ğŸ§ª Comprehensive Testing

### Automated Test Suite

```bash
# Quick validation (< 5 seconds)
python3 scripts/test_microphone_basic.py

# Engine compatibility test
python3 scripts/test_transcription_api.py

# Full system integration (20+ seconds)
python3 scripts/test_complete_system.py

# Installation verification
python3 scripts/validate_installation.py

# Unit tests
python3 -m pytest tests/ -v
```

### Test Coverage

- âœ… **Audio Capture**: Device detection and recording
- âœ… **Speech Recognition**: All 12+ engines tested
- âœ… **Speaker Identification**: All methods validated
- âœ… **Real-time API**: WebSocket functionality
- âœ… **HTTP API**: All endpoints tested
- âœ… **Performance**: Latency and resource usage
- âœ… **Error Handling**: Graceful failure scenarios

## ğŸ› Troubleshooting

### Common Issues

#### ğŸ¤ Audio Problems
```bash
# List available devices
python3 scripts/detect_audio_devices.py --list

# Test specific device
python3 scripts/detect_audio_devices.py --test 2

# Check system audio
arecord -l
aplay -l

# Fix permissions
sudo usermod -a -G audio $USER
```

#### ğŸ§  Model Issues
```bash
# Re-download models
rm -rf models/*.bin
python3 scripts/validate_installation.py

# Check model integrity
ls -la models/
file models/ggml-base.bin
```

#### ğŸŒ API Connection Problems
```bash
# Test external API
curl -X POST https://your-api.com/transcribe \\
  -H "Content-Type: application/json" \\
  -d '{"test": "connection"}'

# Check logs
tail -f logs/combined.log | grep ERROR
```

#### ğŸ’¾ Performance Issues
```bash
# Monitor resources
curl http://localhost:8080/health/detailed

# Use smaller model
WHISPER_MODEL_PATH=./models/ggml-tiny.bin

# Reduce processing frequency
CHUNK_DURATION_MS=5000
```

### Diagnostic Tools

```bash
# System health check
curl http://localhost:8080/health

# Real-time metrics
curl http://localhost:8080/health/detailed

# Speaker identification status
curl http://localhost:8080/speakers/statistics

# Real-time API status
curl http://localhost:8080/realtime/status

# View recent logs
tail -f logs/combined.log
```

## ğŸ¤ Contributing

### Development Setup

```bash
# Fork and clone
git clone https://github.com/your-username/whispersilent.git
cd whispersilent

# Development installation
pip install -r requirements.txt
pip install -e .

# Install development tools
pip install pytest black flake8 mypy

# Run tests
python3 -m pytest tests/ -v
```

### Code Standards

- ğŸ **Python**: PEP 8 compliance, type hints preferred
- ğŸ“ **Commits**: Conventional commits (feat, fix, docs, etc.)
- ğŸ§ª **Testing**: pytest for new features
- ğŸ“– **Documentation**: Comprehensive docstrings
- ğŸ”§ **Configuration**: Environment-based settings

### Contribution Workflow

1. **Fork** the repository
2. **Create** feature branch: `git checkout -b feature/amazing-feature`
3. **Develop** with tests and documentation
4. **Test** thoroughly: `python3 scripts/test_complete_system.py`
5. **Commit** with clear messages: `git commit -m 'feat: add amazing feature'`
6. **Push** to branch: `git push origin feature/amazing-feature`
7. **Create** pull request with detailed description

## ğŸš€ Roadmap

### Upcoming Features

- [ ] ğŸŒ **Multi-language detection** and automatic switching
- [ ] ğŸ“± **Mobile clients** (iOS/Android apps)
- [ ] â˜ï¸ **Cloud deployment** templates (AWS, GCP, Azure)
- [ ] ğŸ¤– **LLM integration** for post-processing and summarization
- [ ] ğŸ“Š **Advanced analytics** with machine learning insights
- [ ] ğŸ” **Enterprise features** (authentication, RBAC, audit logs)
- [ ] ğŸ¥ **Video transcription** support
- [ ] ğŸ”„ **Plugin system** for custom extensions

### Community Features

- [ ] ğŸª **Plugin marketplace** for community extensions
- [ ] ğŸ—ï¸ **Configuration templates** for common use cases
- [ ] ğŸ”— **Third-party integrations** (Slack, Teams, Discord)
- [ ] ğŸ“š **Community documentation** and tutorials

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **[Whisper.cpp](https://github.com/ggerganov/whisper.cpp)** - Optimized Whisper implementation
- **[OpenAI Whisper](https://github.com/openai/whisper)** - Base transcription model
- **[PyAnnote](https://github.com/pyannote/pyannote-audio)** - Speaker diarization framework
- **[SpeechRecognition](https://github.com/Uberi/speech_recognition)** - Python speech recognition library
- **[Seeed VoiceCard](https://github.com/HinTak/seeed-voicecard)** - Raspberry Pi audio driver

## ğŸ“ Support & Community

- ğŸ› **Bug Reports**: [GitHub Issues](https://github.com/your-username/whispersilent/issues)
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/your-username/whispersilent/discussions)  
- ğŸ“– **Documentation**: [Project Wiki](https://github.com/your-username/whispersilent/wiki)
- ğŸ“§ **Contact**: [Maintainer Email](mailto:maintainer@whispersilent.com)

---

**ğŸ¤ WhisperSilent** - Professional real-time transcription with advanced features
*Transform voice to text with precision, intelligence, and scalability*